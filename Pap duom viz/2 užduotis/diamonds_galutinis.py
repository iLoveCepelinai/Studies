# -*- coding: utf-8 -*-
"""Diamonds_galutinis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gCX8VHxPKq4tvMsm3qUl7rEjVOdiTyWP

# PCA

## Duomenu nuskaitymas ir paruošimas

#### Naudojame 'cut' kaip musu target kintamaji
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from keras.datasets import mnist
from sklearn.manifold import MDS, smacof
import seaborn as sns
import plotly.express as px
from matplotlib.ticker import MaxNLocator

from sklearn.metrics.pairwise import pairwise_distances

# Naudojant google colab:
from google.colab import files
uploaded = files.upload()

# Nuskaitomi duomenys
df_visas = pd.read_csv("diamonds.csv")

df_visas = df_visas.drop("color", axis = "columns")
df_visas = df_visas.drop("clarity", axis = "columns")

# istriname pirma stulpeli, nes ten tiesiog ID
df_visas = df_visas.iloc[: ,1:]

"""#### Aprašomoji statistika"""

df_visas.describe()

# Toliau bus lenteles suskirstytos i cut grupes pagal:
# carat
df_visas[['cut', 'carat']].groupby('cut').describe()

# depth
df_visas[['cut', 'depth']].groupby('cut').describe()

# table
df_visas[['cut', 'table']].groupby('cut').describe()

# price
df_visas[['cut', 'price']].groupby('cut').describe()

# x
df_visas[['cut', 'x']].groupby('cut').describe()

# y
df_visas[['cut', 'y']].groupby('cut').describe()

# z
df_visas[['cut', 'z']].groupby('cut').describe()

"""#### Imties ėmimas"""

# Issirenkame po 200 kiekvieno tipo
df = df_visas.groupby('cut').apply(lambda x: x.sample(200, random_state = 1)).reset_index(drop=True)

# Turime po 300 kiekvieno 'cut'
df['cut'].value_counts()

"""### Išskirčių tikrinimas"""

# Staciakampes diagramos pagal cut
sns.set_palette(sns.color_palette(['#d1495b', '#00798c', '#66a182', '#8d96a3', '#957DAD']))

fig, axes = plt.subplots(3, 3, figsize=(20, 20))
sns.boxplot(ax=axes[0, 0], data=df, x='cut', y='carat').set(title = "Carat")
sns.boxplot(ax=axes[0, 1], data=df, x='cut', y='depth').set(title = "Depth")
sns.boxplot(ax=axes[0, 2], data=df, x='cut', y='table').set(title = "Table")
sns.boxplot(ax=axes[1, 0], data=df, x='cut', y='price').set(title = "Price")
sns.boxplot(ax=axes[1, 1], data=df, x='cut', y='x').set(title = "x")
sns.boxplot(ax=axes[1, 2], data=df, x='cut', y='y').set(title = "y")
sns.boxplot(ax=axes[2, 0], data=df, x='cut', y='z').set(title = "z")
fig.delaxes(ax=axes[2,1])
fig.delaxes(ax=axes[2,2])

df.boxplot('price')

def outlier(df):
    dfi = df._get_numeric_data()


    q1 = dfi.quantile(0.25)
    q3 = dfi.quantile(0.75)

    iqr = q3 - q1

    lower_bound = q1 -(3 * iqr) 
    upper_bound = q3 +(3 * iqr)

    return (lower_bound, upper_bound)

# visi Q1 - 3iqr ir Q3 + 3 IQR
outlier(df)

# tik price
price_out = outlier(df)[1][3]
price_out

# Ismetame visas isskirtis:
df.drop(df[df.price >= price_out].index, inplace=True)
max(df.iloc[:,4])

# Isskirtis su carat
df.boxplot('carat')

out_carat = price_out = outlier(df)[1][0]
out_carat

# Ismetame carat isskirti:
df.drop(df[df.carat >= out_carat].index, inplace = True)
max(df.iloc[:,0])

# Isskirtis su depth
df.boxplot('depth')

# Matome isskirtis Q1-3IQR. Saliname kaip anksciau
out_depth = outlier(df)[0][1]
out_depth

# Ismetame carat isskirti:
df.drop(df[df.depth <= out_depth].index, inplace = True)
min(df.iloc[:,2])

# Isskirtis su table
df.boxplot('table')

out_table = outlier(df)[1][2]
df.drop(df[df.table >= out_table].index, inplace = True)
max(df.iloc[:,3])

df

"""Tyrime pirmuoju bandymu mūsų cut grupė 'fair' atrodė, lyg turėtų du klasterius, todėl dabar tikriname, dėl ko 'fair' sudaro du klasterius"""

plt.scatter(df[df["cut"] == "Fair"]["depth"], df[df["cut"] == "Fair"]["table"])
plt.xlabel("Depth")
plt.ylabel("Table")
plt.show()

"""Matome, kad klasė smarkiai atsiskiria pagal depth parametrą, todėl skirstysime į dvi grupes: LD-fair (low depth) ir HD-fair (high depth)"""

backup = df.copy()

df.loc[(df["depth"] >= 63 ) & (df["cut"] == "Fair"), "cut"] = "HD-Fair"
df.loc[(df["depth"] < 63) & (df["cut"] == "Fair"), "cut"] = "LD-Fair"

"""Tap pat iš stačiakampių diagramų matome, kad good ir very good mažai skiriasi, todėl šias grupes sujungiame."""

df.loc[df["cut"] == "Very Good", "cut"] = "Good"

df['cut'].unique()

"""### Koreliacijų matrica"""

df.corr()

"""Matome, kad kovaraintės carat, x, y, z koreliuoja, atmetame y (labai panašus į x, tai nėra didelio skirtumo kurį atmetame), z (turi išskirčių, daugiau koreliuoja ir su likusiomi kovariantėmis)."""

df = df.drop("y", axis = "columns")
df = df.drop("z", axis = "columns")

"""### Duomenų normavimas paga vidurkį ir dispersiją"""

from sklearn.preprocessing import StandardScaler
features = ['carat', 'depth', 'table', 'price', 'x']# stulpeliu pavadinimai
x_ro = df.loc[:, features].values # paimti duomenys be klases
y = df.loc[:,['cut']].values#  paimta klase
x_ = StandardScaler().fit_transform(x_ro) # duomenys normuoti pagal vidurki ir dispersija

df['cut'].unique()

# Sunormuotu duomenu lentele
dfnorm = pd.DataFrame(x_, columns = ['carat', 'depth', 'table', 'price', 'x'])
dfnorm

"""## PCA

### Normuoti duomenys
"""

n_comp = 3 # kiek komponentu norime
pca_list = []
for i in range(n_comp): # sukuriamas list objektas su stulpeliu pavadinimais (pca_1 pca_2 pca_3...)
    pca_list.append("pca_" + str(i+1))

from sklearn.decomposition import PCA
pca = PCA(n_components=n_comp)
principalComponents = pca.fit_transform(x_)
principalDf = pd.DataFrame(data = principalComponents, columns = pca_list)

principalDf

y_pca = df[['cut']].reset_index(drop = True)

finalDf = pd.concat([principalDf, y_pca], axis = 1) # prijungiami klasifikatoriaus reiksmes

finalDf

pca.explained_variance_ratio_

"""#### Vizualizavimas normuotų"""

# Normuoti
PC_values = np.arange(pca.n_components_) + 1
plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.title('Scree PLot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.show()

fig = plt.figure(figsize = (10,10))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)
targets = ['LD-Fair', 'HD-Fair', 'Good', 'Ideal', 'Premium']
colors = ['#d1495b', '#edae49', '#00798c', '#66a182', '#8d96a3']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['cut'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'pca_1']
               , finalDf.loc[indicesToKeep, 'pca_2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()

fig = plt.figure(figsize = (10,10))
ax = plt.axes(projection ="3d")
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_zlabel('Principal Component 3', fontsize = 15)
ax.set_title('3 component PCA', fontsize = 20)
targets = ['LD-Fair', 'HD-Fair', 'Good', 'Ideal', 'Premium']
colors = ['#d1495b', '#edae49', '#00798c', '#66a182', '#8d96a3']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['cut'] == target
    ax.scatter3D(finalDf.loc[indicesToKeep, 'pca_1']
               , finalDf.loc[indicesToKeep, 'pca_2']
               , finalDf.loc[indicesToKeep, 'pca_3']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()

# Papildomas argumetas 3d spalvoms del vientisumo
color_map={"LD-Fair": '#d1495b',
                    "HD-Fair": '#edae49',
                    "Good": '#00798c',
                    "Premium": '#8d96a3',
                    "Ideal": '#66a182'}

px.scatter_3d(finalDf, x='pca_1', y='pca_2', z='pca_3', color='cut',title="3 dimensional PCA",color_discrete_sequence=px.colors.qualitative.Alphabet, color_discrete_map=color_map)

"""### Nenormuoti duomenys"""

pca = PCA(n_components=n_comp)
principalComponents = pca.fit_transform(x_ro)
principalDf = pd.DataFrame(data = principalComponents, columns = pca_list)

finalDf_ro = pd.concat([principalDf, y_pca], axis = 1) # prijungiami klasifikatoriaus reiksmes
print(pca.explained_variance_ratio_)

"""#### Vizualizavimas nenormuotų"""

# Nenormuoti scree plot
PC_values = np.arange(pca.n_components_) + 1
plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.show()

# Nenormuoti taskine diagrama
fig = plt.figure(figsize = (10,10))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)
targets = ['LD-Fair', 'HD-Fair', 'Good', 'Ideal', 'Premium']
colors = ['#d1495b', '#edae49', '#00798c', '#66a182', '#8d96a3']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf_ro['cut'] == target
    ax.scatter(finalDf_ro.loc[indicesToKeep, 'pca_1']
               , finalDf_ro.loc[indicesToKeep, 'pca_2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()

"""# MDS"""

def do_mds(data, labels, n_components = 2,metric=True,n_init=4,max_iter=1000,random_state=3000,
          dissimilarity="euclidean",verbose=0):
    mds=MDS(n_components=n_components, 
          metric=metric, 
          n_init=n_init, 
          max_iter=max_iter, 
          verbose=verbose, 
          eps=0.00001, 
          n_jobs=None, 
          random_state = random_state,
          dissimilarity=dissimilarity)

    data_transformed = mds.fit_transform(data)
    stress = round(mds.stress_,0)
    print('Iterations: ', mds.n_iter_)
    print('Stress: ', stress)

    columns = ["MDS" + str(i) for i in range(1,n_components + 1)]
    toplot = pd.DataFrame(data_transformed,columns=columns)
    toplot["label"] = labels
    
    return toplot, stress

def plot_mds(data,title,ax=None,**kwargs):
    legend = None
    if ax is None:
        fig = plt.figure(figsize=(20, 9))
        ax = fig.add_subplot(1,1,1)
        legend = "auto"
    ax.set_title(title)
    sns.scatterplot(x="MDS1",y="MDS2",hue="label",data=data,alpha = 0.6,s=300,ax=ax,legend=legend,**kwargs)
    
def plot_mds3d(data):
    plot = px.scatter_3d(data, x='MDS1', y='MDS2', z='MDS3',
              color='label',title="3 dimensional MDS (Euclidian distances)", color_discrete_sequence=px.colors.qualitative.Alphabet, color_discrete_map=color_map)
    return plot

"""### Normuoti duomenys"""

# vizualizacija sumazinus dimensija iki 2
sns.set_palette(sns.color_palette(['#d1495b', '#edae49', '#00798c', '#66a182', '#8d96a3']))

dfMDS , _ = do_mds(x_, y)

plot_mds(dfMDS,"2 dimensional MDS (Euclidian distances)")

def stress_plot(data,labels):
    stress = []
    n_dimensions = range(1,data.shape[1])
    for i in n_dimensions:
        _ , stress_value = do_mds(data,labels,n_components=i)
        stress.append(stress_value)

    ax = sns.lineplot(x=n_dimensions,y=stress)
    ax.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax.set_xlabel("n_components")
    ax.set_ylabel("stress")
    ax.set_title("Stress by remaining components")

# matome, kaip stress stiprai padideja tik jeigu noretume sumazinti dim iki 1 (daroma pagal kiekviena stulpeli)
stress_plot(x_, y)

dist_types = ['euclidean','manhattan', 'canberra', 'chebyshev', 'hamming', 'mahalanobis']

fig, ax = plt.subplots(3,2,figsize=(24, 20))
ax = ax.flatten()
              
for i,j in enumerate(dist_types):
    dist = pairwise_distances(x_, metric=j)
    dfMDS, _ = do_mds(dist,y,dissimilarity="precomputed")
    plot_mds(dfMDS,j + " distance",ax[i])  
    ax[i].set_xlabel("")
    ax[i].set_ylabel("")

# n_init nustato kiek kartu atlikti SMACOF algoritma -> grazinamas tik geriausias rezultatas
# matoma, kad naudojant tik viena iteracija siuo atveju randamas tik lokalus minimumas 

n_init = [1,2,3,4]

fig, ax = plt.subplots(2,2,figsize=(20, 16))
ax = ax.flatten()
              
    
for i,j in enumerate(n_init):
    dfMDS, _ = do_mds(x_,y,n_init=j)
    plot_mds(dfMDS,"n_init (number of restarts)=" + str(j),ax[i])  
    ax[i].set_xlabel("")
    ax[i].set_ylabel("")

# vizualizacija sumazinus dimensija iki 3
dfMDS , _ = do_mds(x_,y,n_components=3)

plot_mds3d(dfMDS)

dfMDS

"""### Nenormuoti duomenys"""

# vizualizacija sumazinus dimensija iki 2
sns.set_palette(sns.color_palette(['#d1495b', '#edae49', '#00798c', '#66a182', '#8d96a3']))
dfMDS , _ = do_mds(x_ro, y)

plot_mds(dfMDS,"2 dimensional MDS (Euclidian distances)")

"""# T-SNE"""

# Sukuriame spalvu stulpeli del vientisumo
df1 = df.copy()
df1.loc[df1["cut"] == 'LD-Fair', "cutID"] = "#d1495b"
df1.loc[df1["cut"] == 'HD-Fair', "cutID"] = "#edae49"
df1.loc[df1["cut"] == 'Good', "cutID"] = "#00798c"
df1.loc[df1["cut"] == 'Ideal', "cutID"] = "#66a182"
df1.loc[df1["cut"] == 'Premium', "cutID"] = "#8d96a3"

# Modelio kurimas
from sklearn.manifold import TSNE
import plotly.express as px

tsne = TSNE(n_components=2, random_state=0)
projections = tsne.fit_transform(x_)

# Taskine diagrama
fig = px.scatter(
    projections, x=0, y=1,
    color=df1.cut, color_discrete_sequence=px.colors.qualitative.Alphabet, color_discrete_map=color_map
)
fig.show()

# 3D TSNE
tsne = TSNE(n_components=3, random_state=0)
projections = tsne.fit_transform(x_)

fig = px.scatter_3d(
    projections, x=0, y=1, z=2,
    color=df1.cut, color_discrete_sequence=px.colors.qualitative.Alphabet, color_discrete_map=color_map
)
fig.update_traces(marker_size=7)
fig.show()

projections

df1

data1 = TSNE(n_components=2, perplexity=3).fit_transform(x_)
data2 = TSNE(n_components=2, perplexity=5).fit_transform(x_)
data3 = TSNE(n_components=2, perplexity=8).fit_transform(x_)
data4 = TSNE(n_components=2, perplexity=10).fit_transform(x_)
data5 = TSNE(n_components=2, perplexity=15).fit_transform(x_)
data6 = TSNE(n_components=2, perplexity=20).fit_transform(x_)
data7 = TSNE(n_components=2, perplexity=30).fit_transform(x_)
data8 = TSNE(n_components=2, perplexity=40).fit_transform(x_)
data9 = TSNE(n_components=2, perplexity=50).fit_transform(x_)

plt.rcParams["figure.figsize"] = (20,20)
#plot 1:
plt.subplot(3, 3, 1)
plt.scatter(data1[:, 0], data1[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=3')
#plot 2:
plt.subplot(3, 3, 2)
plt.scatter(data2[:, 0], data2[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=5')
#plot 3:
plt.subplot(3, 3, 3)
plt.scatter(data3[:, 0], data3[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=8')
#plot 4:
plt.subplot(3, 3, 4)
plt.scatter(data4[:, 0], data4[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=10')
#plot 5:
plt.subplot(3, 3, 5)
plt.scatter(data5[:, 0], data5[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=15')
#plot 6:
plt.subplot(3, 3, 6)
plt.scatter(data6[:, 0], data6[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=20')
#plot 7:
plt.subplot(3, 3, 7)
plt.scatter(data7[:, 0], data7[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=30')
#plot 8:
plt.subplot(3, 3, 8)
plt.scatter(data8[:, 0], data8[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=40')
#plot 9:
plt.subplot(3, 3, 9)
plt.scatter(data9[:, 0], data9[:, 1], c=df1.cutID,cmap='tab10')
plt.title('perplexity=50')

"""### Nenormuoti duomenys"""

# Modelio kurimas
from sklearn.manifold import TSNE
import plotly.express as px

tsne = TSNE(n_components=2, random_state=0)
projections = tsne.fit_transform(x_ro)

# Taskine diagrama
fig = px.scatter(
    projections, x=0, y=1,
    color=df1.cut, color_discrete_sequence=px.colors.qualitative.Alphabet, color_discrete_map=color_map
)
fig.show()